{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13RENDA/MCTSinWatermark/blob/main/MCTS_in_WM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KGW repo"
      ],
      "metadata": {
        "id": "brtjGzUiGEaw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Gi1jz3PjxY",
        "outputId": "0abc91eb-ed6a-465a-c9e9-9ab9863ac101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-watermarking'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 336 (delta 20), reused 10 (delta 6), pack-reused 289 (from 1)\u001b[K\n",
            "Receiving objects: 100% (336/336), 12.00 MiB | 20.86 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/jwkirchenbauer/lm-watermarking.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd lm-watermarking"
      ],
      "metadata": {
        "id": "vC7_L2rfhU42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba86032-ce85-4c04-d04c-3e4ee3bc780c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lm-watermarking\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SHi48tKthA1j",
        "outputId": "9b5d9e31-4581-4db1-9a30-65c9e31ede9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio (from -r requirements.txt (line 1))\n",
            "  Downloading gradio-5.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (3.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.44.2)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.19.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting huggingface-hub>=0.25.1 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (1.26.4)\n",
            "Collecting orjson~=3.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (2.9.2)\n",
            "Collecting pydub (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 1)) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r requirements.txt (line 1))\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio->-r requirements.txt (line 1)) (2024.6.1)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio->-r requirements.txt (line 1))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 2)) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.4.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio->-r requirements.txt (line 1)) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio->-r requirements.txt (line 1)) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio->-r requirements.txt (line 1))\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.24.1->gradio->-r requirements.txt (line 1))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 1)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio->-r requirements.txt (line 1)) (2.23.4)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (13.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio->-r requirements.txt (line 1)) (0.1.2)\n",
            "Downloading gradio-5.4.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.7/94.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading orjson-3.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.7.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, tomlkit, semantic-version, ruff, python-multipart, orjson, markupsafe, h11, ffmpy, aiofiles, uvicorn, starlette, huggingface-hub, httpcore, httpx, fastapi, safehttpx, gradio-client, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.4 ffmpy-0.4.0 gradio-5.4.0 gradio-client-1.4.2 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.26.2 markupsafe-2.1.5 orjson-3.10.10 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.1 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python demo_watermark.py --gpt2 facebook/opt-6.7b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2o15npIo__l",
        "outputId": "52828305-62ca-4e36-f3d4-054f47772e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/lm-watermarking/demo_watermark.py\", line 24, in <module>\n",
            "    import gradio as gr\n",
            "ModuleNotFoundError: No module named 'gradio'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KGW Demo"
      ],
      "metadata": {
        "id": "nD5oQYDUtDUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example use of KGW watermark generator\n",
        "# from extended_watermark_processor import WatermarkLogitsProcessor\n",
        "from extended_watermark_processor import WatermarkLogitsProcessor\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList\n",
        "\n",
        "# Assuming you are working with a pre-trained model like 'gpt2'\n",
        "model_name = \"gpt2\"  # Replace with your desired model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) # Load the model\n",
        "new_tokens = 1\n",
        "input_text = \"The quick brown fox jumps over the\"\n",
        "# watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()))\n",
        "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                               gamma=0.25,\n",
        "                                               delta=2.0,\n",
        "                                               seeding_scheme=\"selfhash\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n",
        "# watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
        "#                                                gamma=0.25,\n",
        "#                                                delta=2.0,\n",
        "#                                                seeding_scheme=\"ff-anchored_minhash_prf-4-True-50256\") #equivalent to `ff-anchored_minhash_prf-4-True-15485863`\n",
        "\n",
        "# Note:\n",
        "# You can turn off self-hashing by setting the seeding scheme to `minhash`.\n",
        "\n",
        "tokenized_input = tokenizer(input_text, return_tensors='pt').to(model.device)\n",
        "# note that if the model is on cuda, then the input is on cuda\n",
        "# and thus the watermarking rng is cuda-based.\n",
        "# This is a different generator than the cpu-based rng in pytorch!\n",
        "\n",
        "output_tokens = model.generate(**tokenized_input,\n",
        "                               logits_processor=LogitsProcessorList([watermark_processor]),\n",
        "                               max_new_tokens = 25,\n",
        "                               repetition_penalty=1.1)\n",
        "\n",
        "# if decoder only model, then we need to isolate the\n",
        "# newly generated tokens as only those are watermarked, the input/prompt is not\n",
        "# output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n",
        "\n",
        "output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "bPixBcqoirpX",
        "outputId": "1c1a2dc2-2de7-40f9-bd81-c0ee804c0d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'extended_watermark_processor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5644d1534ce5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example use of KGW watermark generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from extended_watermark_processor import WatermarkLogitsProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mextended_watermark_processor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWatermarkLogitsProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'extended_watermark_processor'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_tokens = model.generate(**tokenized_input,\n",
        "                               logits_processor=LogitsProcessorList([watermark_processor]))\n",
        "print(output_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUiF-EMP1xRx",
        "outputId": "991d362f-25ad-42a0-9830-70df0d4ef2dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  464,  2068,  7586, 21831, 18045,   625,   262,  3355,    11,   475,\n",
            "           339,  1595,   470,   760,   611,   339,   460,   466,   340,    13]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example use of KGW watermark detector\n",
        "# from extended_watermark_processor import WatermarkDetector\n",
        "from extended_watermark_processor import WatermarkDetector\n",
        "\n",
        "# watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
        "#                                         gamma=0.5, # should match original setting\n",
        "#                                         seeding_scheme=\"simple_1\", # should match original setting\n",
        "#                                         device=model.device, # must match the original rng device type\n",
        "#                                         tokenizer=tokenizer,\n",
        "#                                         z_threshold=4.0,\n",
        "#                                         normalizers=[])\n",
        "\n",
        "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                        gamma=0.25, # should match original setting\n",
        "                                        seeding_scheme=\"selfhash\", # should match original setting\n",
        "                                        device=model.device, # must match the original rng device type\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        z_threshold=4.0,\n",
        "                                        normalizers=[],\n",
        "                                        ignore_repeated_ngrams=True)\n",
        "\n",
        "# watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
        "#                                         gamma=0.25, # should match original setting\n",
        "#                                         seeding_scheme=\"ff-anchored_minhash_prf-4-True-50256\", # should match original setting\n",
        "#                                         device=model.device, # must match the original rng device type\n",
        "#                                         tokenizer=tokenizer,\n",
        "#                                         z_threshold=4.0,\n",
        "#                                         normalizers=[],\n",
        "#                                         ignore_repeated_ngrams=True)\n",
        "\n",
        "score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze\n",
        "print(score_dict)"
      ],
      "metadata": {
        "id": "cx26E1Xij3vU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34c3c10b-fa8b-4496-9d94-15a5d86fab66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'num_tokens_scored': 21, 'num_green_tokens': 7, 'green_fraction': 0.3333333333333333, 'z_score': 0.8819171036881969, 'p_value': 0.1889108185500319, 'z_score_at_T': tensor([-0.5774, -0.8165, -1.0000, -1.1547, -0.2582,  0.4714,  0.2182,  0.0000,\n",
            "        -0.1925, -0.3651,  0.1741,  0.6667,  0.4804,  0.3086,  0.1491,  0.5774,\n",
            "         0.4201,  0.8165,  0.6623,  1.0328,  0.8819]), 'prediction': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS in Watermark"
      ],
      "metadata": {
        "id": "A2ai7NCAF68q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList\n",
        "from extended_watermark_processor import WatermarkLogitsProcessor, WatermarkDetector  # Assuming this is the main class in your file\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "# Load model and tokenizer (you can replace 'gpt2' with a larger model if needed)\n",
        "model_name = 'gpt2'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "# Helper function for token entropy calculation\n",
        "def calculate_entropy(logits):\n",
        "    probabilities = F.softmax(logits, dim=-1)\n",
        "    log_probabilities = F.log_softmax(logits, dim=-1)\n",
        "    entropy = -(probabilities * log_probabilities).sum(-1)  # Summing over the vocabulary dimension\n",
        "    return entropy\n",
        "\n",
        "# Helper function for perplexity calculation\n",
        "def compute_perplexity(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs.input_ids\n",
        "\n",
        "    # Calculate cross entropy loss\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "        loss = outputs.loss  # Cross entropy loss\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = torch.exp(loss)\n",
        "    return perplexity.item()\n",
        "\n",
        "# Initialize the watermark processor\n",
        "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                               gamma=0.25,  # Adjust gamma if needed\n",
        "                                               delta=2.0,  # Adjust delta if needed\n",
        "                                               seeding_scheme=\"selfhash\")  # Adjust seeding_scheme if needed\n",
        "\n",
        "# Initialize the watermark detector\n",
        "\n",
        "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                       gamma=0.25,\n",
        "                                       seeding_scheme=\"selfhash\",\n",
        "                                       device=model.device,\n",
        "                                       tokenizer=tokenizer,\n",
        "                                       z_threshold=4.0,\n",
        "                                       normalizers=[],\n",
        "                                       ignore_repeated_ngrams=True)\n",
        "\n",
        "# Helper function for Monte Carlo Tree Search Node\n",
        "class MCTSNode:\n",
        "    def __init__(self, text, is_watermarked=False, parent=None):\n",
        "        self.text = text\n",
        "        self.is_watermarked = is_watermarked\n",
        "        self.parent = parent\n",
        "        self.children = []\n",
        "        self.visits = 0\n",
        "        self.value = 0\n",
        "\n",
        "    def add_child(self, child_node):\n",
        "        self.children.append(child_node)\n",
        "\n",
        "# MCTS algorithm\n",
        "class MCTS:\n",
        "    def __init__(self, model, tokenizer, baseline_text, max_depth=10):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.baseline_text = baseline_text\n",
        "        self.baseline_encoding = self.tokenizer.encode(baseline_text, return_tensors='pt')\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def expand(self, node, current_depth):\n",
        "        if current_depth >= self.max_depth:\n",
        "            return node\n",
        "        # Tokenize the current text to generate logits for the next possible tokens\n",
        "        input_ids = self.tokenizer.encode(node.text, return_tensors='pt', padding=True)\n",
        "        attention_mask = input_ids.new_ones(input_ids.shape)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_ids, return_dict=True)\n",
        "            logits = outputs.logits[:, -1, :]  # Get logits for the last generated token\n",
        "            entropy = calculate_entropy(logits)\n",
        "\n",
        "        # Sort tokens by entropy (descending order) and expand high-entropy tokens\n",
        "\n",
        "        # k = min(5, entropy.shape[-1])\n",
        "        # topk = torch.topk(entropy, k=k, dim=-1)\n",
        "        # top_tokens = topk.indices.tolist()\n",
        "\n",
        "        watermark_processor.rng = torch.Generator(device=input_ids.device)if watermark_processor.rng is None else watermark_processor.rng\n",
        "        # Generate greenlist based on current input sequence\n",
        "        greenlist_ids = watermark_processor._get_greenlist_ids(input_ids[0])\n",
        "\n",
        "        # Generate the non-watermarked token (greedy choice)每一个子节点为前文+新词\n",
        "        output_tokens_without_watermark = self.model.generate(input_ids, attention_mask = attention_mask, max_new_tokens=1)\n",
        "        non_watermarked_token_id = output_tokens_without_watermark[0][-1].item()\n",
        "        child_text_no_watermark = node.text + self.tokenizer.decode([non_watermarked_token_id], skip_special_tokens=True)\n",
        "        print(f\"leaf node w/o WM:{child_text_no_watermark}\")\n",
        "        # print(f\"leaf node w/o WM:{self.tokenizer.decode([non_watermarked_token_id])}\")\n",
        "        child_node_no_watermark = MCTSNode(child_text_no_watermark, is_watermarked=False, parent=node)\n",
        "        node.add_child(child_node_no_watermark)\n",
        "\n",
        "        # Remove the non-watermarked token from the greenlist\n",
        "        if non_watermarked_token_id in greenlist_ids:\n",
        "            greenlist_ids = greenlist_ids[greenlist_ids != non_watermarked_token_id]\n",
        "\n",
        "\n",
        "        # Expand into both watermarked and non-watermarked versions\n",
        "        # green_list_ids = watermark_processor._get_greenlist_ids(input_ids, logits)\n",
        "        for wm_token_id in greenlist_ids[:5]:\n",
        "            # Generate the watermarked version\n",
        "            # input_ids_with_watermark = self.tokenizer.encode(node.text + token, return_tensors='pt')\n",
        "            # with torch.no_grad():\n",
        "            #     output_tokens_with_watermark = self.model.generate(**{'input_ids': input_ids_with_watermark},\n",
        "            #                                                          logits_processor=LogitsProcessorList([watermark_processor]),\n",
        "            #                                                          max_new_tokens=1)\n",
        "            child_text_with_watermark = node.text + self.tokenizer.decode([wm_token_id], skip_special_tokens=True)\n",
        "            print(f\"leaf node w/ WM:{child_text_with_watermark}\")\n",
        "            # print(f\"leaf node w/o WM:{self.tokenizer.decode([wm_token_id])}\")\n",
        "            child_node_with_watermark = MCTSNode(child_text_with_watermark, is_watermarked=True,parent=node)\n",
        "            node.add_child(child_node_with_watermark)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def simulate(self, node):\n",
        "        # Simulate until we reach the end or max depth用水印模型模拟\n",
        "        tokenized_input = self.tokenizer(node.text, return_tensors=\"pt\", padding=True).to(model.device)\n",
        "        # attention_mask = tokenized_input[\"attention_mask\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_outputs = self.model.generate(**tokenized_input,\n",
        "                                                    logits_processor=LogitsProcessorList([watermark_processor]),\n",
        "                                                    max_new_tokens=3,\n",
        "                                                    repetition_penalty=1.1)\n",
        "\n",
        "        generated_text = self.tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
        "        print(f\"simulation result: {generated_text}\")\n",
        "\n",
        "        # Compute reward: combination of semantic similarity and watermark detectability\n",
        "        reward = self.compute_reward(node, generated_text)\n",
        "        return reward\n",
        "\n",
        "    def compute_reward(self, node, generated_text):\n",
        "        '''\n",
        "        # Compute cosine similarity for semantic preservation\n",
        "\n",
        "        baseline_encoding = self.tokenizer.encode(baseline_text[0], return_tensors='pt')\n",
        "        generated_encoding = self.tokenizer.encode(generated_text[0], return_tensors='pt')\n",
        "        cos_sim = F.cosine_similarity(baseline_encoding.float(), generated_encoding.float(), dim=-1)\n",
        "\n",
        "        # Compute watermark detectability using WatermarkDetector\n",
        "        score_dict = watermark_detector.detect(generated_text)\n",
        "        detectability = score_dict['z_score']  # Or another relevant metric from score_dict\n",
        "\n",
        "        # Reward is a combination of semantic similarity and watermark detectability\n",
        "        alpha = 0  # Adjust the weight for detectability\n",
        "        reward = cos_sim.item() + alpha * detectability\n",
        "        '''\n",
        "        perplexity = compute_perplexity(self.model, self.tokenizer, generated_text)\n",
        "        reward = -perplexity\n",
        "        if not node.is_watermarked:\n",
        "            reward *= 8\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def backpropagate(self, node, reward):\n",
        "        # Backpropagate the reward to update the node's value\n",
        "        # if node.parent != None:\n",
        "        #   backpropagate(node.parent, reward)\n",
        "\n",
        "        # node.visits += 1\n",
        "        # if(node.value == float('-inf')):\n",
        "        #   node.value = reward\n",
        "        # else:\n",
        "        #   node.value += reward\n",
        "        # print(f\"root: {node.text}; value:{node.value/node.visits if node.visits > 0 else float('-inf')}\")\n",
        "        if node.parent is not None:  # Check if node is not None before accessing parent\n",
        "          self.backpropagate(node.parent, reward)\n",
        "        node.visits += 1\n",
        "        if(node.value == float('-inf')):\n",
        "            node.value = reward\n",
        "        else:\n",
        "            node.value += reward\n",
        "          # print(f\"reward: {reward}\")\n",
        "        print(f\"parent: {node.text};   {node.value/node.visits if node.visits > 0 else float('-inf')}\")\n",
        "\n",
        "    def search(self, root):\n",
        "        for _ in range(50):  # Number of iterations\n",
        "            node = root\n",
        "            current_depth = 0\n",
        "            print(f\"root: {node.text};   {node.value/node.visits if node.visits > 0 else float('-inf')}\")\n",
        "\n",
        "            # Selection and Expansion\n",
        "            while node.children and current_depth < self.max_depth:\n",
        "                # UCB1 formula to select child\n",
        "                node = max(node.children, key=lambda n: n.value / (n.visits + 1) + math.sqrt(2 * math.log(node.visits + 1) / (n.visits + 1)))\n",
        "                current_depth += 1\n",
        "\n",
        "            # Expansion\n",
        "            if current_depth < self.max_depth:\n",
        "              print(\"expansion:\")\n",
        "              node = self.expand(node, current_depth)\n",
        "\n",
        "            # Simulation\n",
        "            if node.parent is not None:\n",
        "              reward = self.simulate(node)\n",
        "              print(f\"reward: {reward}\")\n",
        "              # Backpropagation\n",
        "              print(\"backpropagate: \")\n",
        "              self.backpropagate(node, reward)\n",
        "\n",
        "        best_node = root\n",
        "        while best_node.children:\n",
        "          best_node = max(best_node.children, key=lambda n: n.value / n.visits if n.visits > 0 else float('inf'))\n",
        "        return best_node.text\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "prompt = \"I'm so happy\"\n",
        "tokenized_input = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "attention_mask = tokenized_input[\"attention_mask\"]\n",
        "\n",
        "# Generate a baseline response\n",
        "baseline_response = model.generate(tokenizer.encode(prompt, return_tensors='pt'), attention_mask=attention_mask, max_new_tokens=8, repetition_penalty=1.1)\n",
        "baseline_text = tokenizer.decode(baseline_response[0], skip_special_tokens=True)\n",
        "\n",
        "# Initialize MCTS and perform search\n",
        "root = MCTSNode(text=prompt)\n",
        "mcts = MCTS(model, tokenizer, baseline_text, max_depth=5)\n",
        "best_response = mcts.search(root)\n",
        "print(f\"baseline response: {baseline_text}\")\n",
        "print(f\"Best response with watermark: {best_response}\")\n"
      ],
      "metadata": {
        "id": "wiv1rDMl1hDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09d631ae-8cad-471a-b2d3-87e9fb458578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root: I'm so happy;   -inf\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to\n",
            "leaf node w/ WM:I'm so happy ambulance\n",
            "leaf node w/ WM:I'm so happy meet\n",
            "leaf node w/ WM:I'm so happy merger\n",
            "leaf node w/ WM:I'm so happysych\n",
            "leaf node w/ WM:I'm so happyenezuel\n",
            "root: I'm so happy;   -inf\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be\n",
            "leaf node w/ WM:I'm so happy to embryo\n",
            "leaf node w/ WM:I'm so happy toctuary\n",
            "leaf node w/ WM:I'm so happy to reassuring\n",
            "leaf node w/ WM:I'm so happy to Actually\n",
            "leaf node w/ WM:I'm so happy to********\n",
            "simulation result: I'm so happy to see it is\n",
            "reward: -182.1492462158203\n",
            "back\n",
            "parent: I'm so happy;   -182.1492462158203\n",
            "parent: I'm so happy to;   -182.1492462158203\n",
            "root: I'm so happy;   -182.1492462158203\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy ambulance crews\n",
            "leaf node w/ WM:I'm so happy ambulance Samantha\n",
            "leaf node w/ WM:I'm so happy ambulance loves\n",
            "leaf node w/ WM:I'm so happy ambulance Or\n",
            "leaf node w/ WM:I'm so happy ambulancesect\n",
            "leaf node w/ WM:I'm so happy ambulance cliffs\n",
            "simulation result: I'm so happy ambulance crews have been\n",
            "reward: -184.00685119628906\n",
            "back\n",
            "parent: I'm so happy;   -183.0780487060547\n",
            "parent: I'm so happy ambulance;   -184.00685119628906\n",
            "root: I'm so happy;   -183.0780487060547\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet you\n",
            "leaf node w/ WM:I'm so happy meet Or\n",
            "leaf node w/ WM:I'm so happy meet sigh\n",
            "leaf node w/ WM:I'm so happy meet Shakespeare\n",
            "leaf node w/ WM:I'm so happy meetodi\n",
            "leaf node w/ WM:I'm so happy meet Barbie\n",
            "simulation result: I'm so happy meet you! You\n",
            "reward: -93.16919708251953\n",
            "back\n",
            "parent: I'm so happy;   -153.10843149820963\n",
            "parent: I'm so happy meet;   -93.16919708251953\n",
            "root: I'm so happy;   -153.10843149820963\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy merger with\n",
            "leaf node w/ WM:I'm so happy mergerberra\n",
            "leaf node w/ WM:I'm so happy merger z\n",
            "leaf node w/ WM:I'm so happy merger actresses\n",
            "leaf node w/ WM:I'm so happy merger intim\n",
            "leaf node w/ WM:I'm so happy merger maneu\n",
            "simulation result: I'm so happy merger and we've\n",
            "reward: -302.3866882324219\n",
            "back\n",
            "parent: I'm so happy;   -190.4279956817627\n",
            "parent: I'm so happy merger;   -302.3866882324219\n",
            "root: I'm so happy;   -190.4279956817627\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happysych,\n",
            "leaf node w/ WM:I'm so happysych Vish\n",
            "leaf node w/ WM:I'm so happysychKR\n",
            "leaf node w/ WM:I'm so happysych五\n",
            "leaf node w/ WM:I'm so happysych circum\n",
            "leaf node w/ WM:I'm so happysych oak\n",
            "simulation result: I'm so happysych! I'll\n",
            "reward: -262.87762451171875\n",
            "back\n",
            "parent: I'm so happy;   -204.9179214477539\n",
            "parent: I'm so happysych;   -262.87762451171875\n",
            "root: I'm so happy;   -204.9179214477539\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happyenezuela\n",
            "leaf node w/ WM:I'm so happyenezuel annot\n",
            "leaf node w/ WM:I'm so happyenezuel Hue\n",
            "leaf node w/ WM:I'm so happyenezuel near\n",
            "leaf node w/ WM:I'm so happyenezuelHi\n",
            "leaf node w/ WM:I'm so happyenezuelbiased\n",
            "simulation result: I'm so happyenezuela's economy\n",
            "reward: -219.67295837402344\n",
            "back\n",
            "parent: I'm so happy;   -207.37709426879883\n",
            "parent: I'm so happyenezuel;   -219.67295837402344\n",
            "root: I'm so happy;   -207.37709426879883\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet you,\n",
            "leaf node w/ WM:I'm so happy meet you independ\n",
            "leaf node w/ WM:I'm so happy meet you sourcing\n",
            "leaf node w/ WM:I'm so happy meet youá\n",
            "leaf node w/ WM:I'm so happy meet you Penalty\n",
            "leaf node w/ WM:I'm so happy meet you insisting\n",
            "simulation result: I'm so happy meet you! You're\n",
            "reward: -476.9402770996094\n",
            "back\n",
            "parent: I'm so happy;   -245.88612038748605\n",
            "parent: I'm so happy meet;   -285.05473709106445\n",
            "parent: I'm so happy meet you;   -476.9402770996094\n",
            "root: I'm so happy;   -245.88612038748605\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here\n",
            "leaf node w/ WM:I'm so happy to be teenage\n",
            "leaf node w/ WM:I'm so happy to be typew\n",
            "leaf node w/ WM:I'm so happy to be SH\n",
            "leaf node w/ WM:I'm so happy to bepled\n",
            "leaf node w/ WM:I'm so happy to beamer\n",
            "simulation result: I'm so happy to be here with you\n",
            "reward: -79.95856475830078\n",
            "back\n",
            "parent: I'm so happy;   -225.1451759338379\n",
            "parent: I'm so happy to;   -131.05390548706055\n",
            "parent: I'm so happy to be;   -79.95856475830078\n",
            "root: I'm so happy;   -225.1451759338379\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to embryo a\n",
            "leaf node w/ WM:I'm so happy to embryoarag\n",
            "leaf node w/ WM:I'm so happy to embryo 28\n",
            "leaf node w/ WM:I'm so happy to embryo immortal\n",
            "leaf node w/ WM:I'm so happy to embryoifferent\n",
            "leaf node w/ WM:I'm so happy to embryo impecc\n",
            "simulation result: I'm so happy to embryo with you!\"\n",
            "reward: -245.0378875732422\n",
            "back\n",
            "parent: I'm so happy;   -227.35547722710504\n",
            "parent: I'm so happy to;   -169.04856618245444\n",
            "parent: I'm so happy to embryo;   -245.0378875732422\n",
            "root: I'm so happy;   -227.35547722710504\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy ambulance crews are\n",
            "leaf node w/ WM:I'm so happy ambulance crewsKeefe\n",
            "leaf node w/ WM:I'm so happy ambulance crews Fallon\n",
            "leaf node w/ WM:I'm so happy ambulance crews differential\n",
            "leaf node w/ WM:I'm so happy ambulance crews\u0010\n",
            "leaf node w/ WM:I'm so happy ambulance crews Saudi\n",
            "simulation result: I'm so happy ambulance crews have been here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -1026.65185546875\n",
            "back\n",
            "parent: I'm so happy;   -307.28511505126954\n",
            "parent: I'm so happy ambulance;   -605.3293533325195\n",
            "parent: I'm so happy ambulance crews;   -1026.65185546875\n",
            "root: I'm so happy;   -307.28511505126954\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happyenezuela is\n",
            "leaf node w/ WM:I'm so happyenezuelaChest\n",
            "leaf node w/ WM:I'm so happyenezuela insomnia\n",
            "leaf node w/ WM:I'm so happyenezuela Falls\n",
            "leaf node w/ WM:I'm so happyenezuelaBrown\n",
            "leaf node w/ WM:I'm so happyenezuela fmt\n",
            "simulation result: I'm so happyenezuela's economy is\n",
            "reward: -974.3021850585938\n",
            "back\n",
            "parent: I'm so happy;   -367.9230305064808\n",
            "parent: I'm so happyenezuel;   -596.9875717163086\n",
            "parent: I'm so happyenezuela;   -974.3021850585938\n",
            "root: I'm so happy;   -367.9230305064808\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy toctuary for\n",
            "leaf node w/ WM:I'm so happy toctuary Odyssey\n",
            "leaf node w/ WM:I'm so happy toctuary genomic\n",
            "leaf node w/ WM:I'm so happy toctuaryRN\n",
            "leaf node w/ WM:I'm so happy toctuary thief\n",
            "leaf node w/ WM:I'm so happy toctuary TL\n",
            "simulation result: I'm so happy toctuary in the face\n",
            "reward: -690.5859375\n",
            "back\n",
            "parent: I'm so happy;   -394.8116060892741\n",
            "parent: I'm so happy to;   -299.4329090118408\n",
            "parent: I'm so happy toctuary;   -690.5859375\n",
            "root: I'm so happy;   -394.8116060892741\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happysych, I\n",
            "leaf node w/ WM:I'm so happysych, fireball\n",
            "leaf node w/ WM:I'm so happysych, Freder\n",
            "leaf node w/ WM:I'm so happysych, prominently\n",
            "leaf node w/ WM:I'm so happysych, handed\n",
            "leaf node w/ WM:I'm so happysych,toc\n",
            "simulation result: I'm so happysych, you know how\n",
            "reward: -1519.7130126953125\n",
            "back\n",
            "parent: I'm so happy;   -481.3424835205078\n",
            "parent: I'm so happysych;   -891.2953186035156\n",
            "parent: I'm so happysych,;   -1519.7130126953125\n",
            "root: I'm so happy;   -481.3424835205078\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy merger with the\n",
            "leaf node w/ WM:I'm so happy merger with>>>\n",
            "leaf node w/ WM:I'm so happy merger withPlug\n",
            "leaf node w/ WM:I'm so happy merger withobbies\n",
            "leaf node w/ WM:I'm so happy merger with corridors\n",
            "leaf node w/ WM:I'm so happy merger with500\n",
            "simulation result: I'm so happy merger with the new company\n",
            "reward: -1801.040771484375\n",
            "back\n",
            "parent: I'm so happy;   -575.6066469464984\n",
            "parent: I'm so happy merger;   -1051.7137298583984\n",
            "parent: I'm so happy merger with;   -1801.040771484375\n",
            "root: I'm so happy;   -575.6066469464984\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet Orson\n",
            "leaf node w/ WM:I'm so happy meet Or oriented\n",
            "leaf node w/ WM:I'm so happy meet Or annoyance\n",
            "leaf node w/ WM:I'm so happy meet Or 317\n",
            "leaf node w/ WM:I'm so happy meet Orquel\n",
            "leaf node w/ WM:I'm so happy meet Or contingency\n",
            "simulation result: I'm so happy meet Orphan Black!\n",
            "reward: -211.7509002685547\n",
            "back\n",
            "parent: I'm so happy;   -551.3495971679688\n",
            "parent: I'm so happy meet;   -260.62012481689453\n",
            "parent: I'm so happy meet Or;   -211.7509002685547\n",
            "root: I'm so happy;   -551.3495971679688\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet sighs\n",
            "leaf node w/ WM:I'm so happy meet sigh SM\n",
            "leaf node w/ WM:I'm so happy meet sigh worry\n",
            "leaf node w/ WM:I'm so happy meet sigh Papers\n",
            "leaf node w/ WM:I'm so happy meet sighcurrently\n",
            "leaf node w/ WM:I'm so happy meet sigh chang\n",
            "simulation result: I'm so happy meet sighs and gigg\n",
            "reward: -469.30853271484375\n",
            "back\n",
            "parent: I'm so happy;   -546.2220306396484\n",
            "parent: I'm so happy meet;   -312.79222679138184\n",
            "parent: I'm so happy meet sigh;   -469.30853271484375\n",
            "root: I'm so happy;   -546.2220306396484\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to reassuring you\n",
            "leaf node w/ WM:I'm so happy to reassuringitol\n",
            "leaf node w/ WM:I'm so happy to reassuring lively\n",
            "leaf node w/ WM:I'm so happy to reassuring Tax\n",
            "leaf node w/ WM:I'm so happy to reassuring aerial\n",
            "leaf node w/ WM:I'm so happy to reassuring Baron\n",
            "simulation result: I'm so happy to reassuring you that your\n",
            "reward: -114.9885482788086\n",
            "back\n",
            "parent: I'm so happy;   -520.8553552066578\n",
            "parent: I'm so happy to;   -262.5440368652344\n",
            "parent: I'm so happy to reassuring;   -114.9885482788086\n",
            "root: I'm so happy;   -520.8553552066578\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to Actually,\n",
            "leaf node w/ WM:I'm so happy to Actually pand\n",
            "leaf node w/ WM:I'm so happy to Actually arg\n",
            "leaf node w/ WM:I'm so happy to Actuallybender\n",
            "leaf node w/ WM:I'm so happy to Actuallyansas\n",
            "leaf node w/ WM:I'm so happy to Actually majestic\n",
            "simulation result: I'm so happy to Actually I've been\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -109.7010726928711\n",
            "back\n",
            "parent: I'm so happy;   -498.0134506225586\n",
            "parent: I'm so happy to;   -237.07020950317383\n",
            "parent: I'm so happy to Actually;   -109.7010726928711\n",
            "root: I'm so happy;   -498.0134506225586\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to**********\n",
            "leaf node w/ WM:I'm so happy to********171\n",
            "leaf node w/ WM:I'm so happy to******** wine\n",
            "leaf node w/ WM:I'm so happy to******** accelerating\n",
            "leaf node w/ WM:I'm so happy to******** Reconstruction\n",
            "leaf node w/ WM:I'm so happy to********NC\n",
            "simulation result: I'm so happy to**********! I\n",
            "reward: -178.83358764648438\n",
            "back\n",
            "parent: I'm so happy;   -481.2145104659231\n",
            "parent: I'm so happy to;   -228.75069209507532\n",
            "parent: I'm so happy to********;   -178.83358764648438\n",
            "root: I'm so happy;   -481.2145104659231\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here.\n",
            "leaf node w/ WM:I'm so happy to be hereVisit\n",
            "leaf node w/ WM:I'm so happy to be here deteriorating\n",
            "leaf node w/ WM:I'm so happy to be here Garmin\n",
            "leaf node w/ WM:I'm so happy to be here Preservation\n",
            "leaf node w/ WM:I'm so happy to be here kidnapping\n",
            "simulation result: I'm so happy to be here with you!\"\n",
            "reward: -82.11121368408203\n",
            "back\n",
            "parent: I'm so happy;   -461.2593456268311\n",
            "parent: I'm so happy to;   -210.42075729370117\n",
            "parent: I'm so happy to be;   -81.0348892211914\n",
            "parent: I'm so happy to be here;   -82.11121368408203\n",
            "root: I'm so happy;   -461.2593456268311\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be teenage again\n",
            "leaf node w/ WM:I'm so happy to be teenageaccess\n",
            "leaf node w/ WM:I'm so happy to be teenage Bruins\n",
            "leaf node w/ WM:I'm so happy to be teenageamaru\n",
            "leaf node w/ WM:I'm so happy to be teenage====\n",
            "leaf node w/ WM:I'm so happy to be teenage 37\n",
            "simulation result: I'm so happy to be teenage! I think\n",
            "reward: -71.63115692138672\n",
            "back\n",
            "parent: I'm so happy;   -442.70562235514325\n",
            "parent: I'm so happy to;   -194.99969058566623\n",
            "parent: I'm so happy to be;   -77.90031178792317\n",
            "parent: I'm so happy to be teenage;   -71.63115692138672\n",
            "root: I'm so happy;   -442.70562235514325\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to Actually, I\n",
            "leaf node w/ WM:I'm so happy to Actually, sacr\n",
            "leaf node w/ WM:I'm so happy to Actually, turrets\n",
            "leaf node w/ WM:I'm so happy to Actually, lessen\n",
            "leaf node w/ WM:I'm so happy to Actually,umbnails\n",
            "leaf node w/ WM:I'm so happy to Actually, Xen\n",
            "simulation result: I'm so happy to Actually, I've been\n",
            "reward: -501.642578125\n",
            "back\n",
            "parent: I'm so happy;   -445.3845748901367\n",
            "parent: I'm so happy to;   -225.66397933959962\n",
            "parent: I'm so happy to Actually;   -305.67182540893555\n",
            "parent: I'm so happy to Actually,;   -501.642578125\n",
            "root: I'm so happy;   -445.3845748901367\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to reassuring you that\n",
            "leaf node w/ WM:I'm so happy to reassuring you deterior\n",
            "leaf node w/ WM:I'm so happy to reassuring you valve\n",
            "leaf node w/ WM:I'm so happy to reassuring you nodd\n",
            "leaf node w/ WM:I'm so happy to reassuring youfighter\n",
            "leaf node w/ WM:I'm so happy to reassuring youhetically\n",
            "simulation result: I'm so happy to reassuring you that your new\n",
            "reward: -884.0169677734375\n",
            "back\n",
            "parent: I'm so happy;   -464.4555484937585\n",
            "parent: I'm so happy to;   -285.51425101540303\n",
            "parent: I'm so happy to reassuring;   -499.50275802612305\n",
            "parent: I'm so happy to reassuring you;   -884.0169677734375\n",
            "root: I'm so happy;   -464.4555484937585\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet Shakespeare.\n",
            "leaf node w/ WM:I'm so happy meet Shakespeare Initially\n",
            "leaf node w/ WM:I'm so happy meet Shakespeareifle\n",
            "leaf node w/ WM:I'm so happy meet ShakespeareSurv\n",
            "leaf node w/ WM:I'm so happy meet Shakespeareotrop\n",
            "leaf node w/ WM:I'm so happy meet Shakespeare236\n",
            "simulation result: I'm so happy meet Shakespeare! He's\n",
            "reward: -253.76724243164062\n",
            "back\n",
            "parent: I'm so happy;   -455.6768690745036\n",
            "parent: I'm so happy meet;   -300.9872299194336\n",
            "parent: I'm so happy meet Shakespeare;   -253.76724243164062\n",
            "root: I'm so happy;   -455.6768690745036\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meetodi.\n",
            "leaf node w/ WM:I'm so happy meetodi Bentley\n",
            "leaf node w/ WM:I'm so happy meetodi LF\n",
            "leaf node w/ WM:I'm so happy meetodi persuaded\n",
            "leaf node w/ WM:I'm so happy meetodi taco\n",
            "leaf node w/ WM:I'm so happy meetodi marrow\n",
            "simulation result: I'm so happy meetodi! I'll\n",
            "reward: -495.74365234375\n",
            "back\n",
            "parent: I'm so happy;   -457.27954040527345\n",
            "parent: I'm so happy meet;   -333.44663365681964\n",
            "parent: I'm so happy meetodi;   -495.74365234375\n",
            "root: I'm so happy;   -457.27954040527345\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be typewritten\n",
            "leaf node w/ WM:I'm so happy to be typew double\n",
            "leaf node w/ WM:I'm so happy to be typew wallets\n",
            "leaf node w/ WM:I'm so happy to be typew chirop\n",
            "leaf node w/ WM:I'm so happy to be typew Wh\n",
            "leaf node w/ WM:I'm so happy to be typew mushroom\n",
            "simulation result: I'm so happy to be typewritten by the\n",
            "reward: -67.5398941040039\n",
            "back\n",
            "parent: I'm so happy;   -442.28955400907074\n",
            "parent: I'm so happy to;   -267.34972127278644\n",
            "parent: I'm so happy to be;   -75.31020736694336\n",
            "parent: I'm so happy to be typew;   -67.5398941040039\n",
            "root: I'm so happy;   -442.28955400907074\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be SHO\n",
            "leaf node w/ WM:I'm so happy to be SH explorer\n",
            "leaf node w/ WM:I'm so happy to be SH Died\n",
            "leaf node w/ WM:I'm so happy to be SHmedium\n",
            "leaf node w/ WM:I'm so happy to be SH very\n",
            "leaf node w/ WM:I'm so happy to be SHannah\n",
            "simulation result: I'm so happy to be SHUT UP.\n",
            "reward: -45.574859619140625\n",
            "back\n",
            "parent: I'm so happy;   -427.59641717981407\n",
            "parent: I'm so happy to;   -250.2901165301983\n",
            "parent: I'm so happy to be;   -69.3631378173828\n",
            "parent: I'm so happy to be SH;   -45.574859619140625\n",
            "root: I'm so happy;   -427.59641717981407\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to bepled to\n",
            "leaf node w/ WM:I'm so happy to bepled fueled\n",
            "leaf node w/ WM:I'm so happy to bepled cheering\n",
            "leaf node w/ WM:I'm so happy to bepledlatest\n",
            "leaf node w/ WM:I'm so happy to bepledinterface\n",
            "leaf node w/ WM:I'm so happy to bepled Advisor\n",
            "simulation result: I'm so happy to bepled.\"\n",
            "\n",
            "reward: -139.14398193359375\n",
            "back\n",
            "parent: I'm so happy;   -417.29454449244906\n",
            "parent: I'm so happy to;   -242.35110691615515\n",
            "parent: I'm so happy to be;   -80.99327850341797\n",
            "parent: I'm so happy to bepled;   -139.14398193359375\n",
            "root: I'm so happy;   -417.29454449244906\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to beamer.\n",
            "leaf node w/ WM:I'm so happy to beamerURA\n",
            "leaf node w/ WM:I'm so happy to beamer squeezing\n",
            "leaf node w/ WM:I'm so happy to beamer arra\n",
            "leaf node w/ WM:I'm so happy to beamer��\n",
            "leaf node w/ WM:I'm so happy to beamerRegister\n",
            "simulation result: I'm so happy to beamer! I think\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -71.11077117919922\n",
            "back\n",
            "parent: I'm so happy;   -405.35717299888876\n",
            "parent: I'm so happy to;   -230.9350845336914\n",
            "parent: I'm so happy to be;   -79.58149174281529\n",
            "parent: I'm so happy to beamer;   -71.11077117919922\n",
            "root: I'm so happy;   -405.35717299888876\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be SHOOT\n",
            "leaf node w/ WM:I'm so happy to be SHO).\n",
            "leaf node w/ WM:I'm so happy to be SHOriott\n",
            "leaf node w/ WM:I'm so happy to be SHO time\n",
            "leaf node w/ WM:I'm so happy to be SHO Ac\n",
            "leaf node w/ WM:I'm so happy to be SHOkn\n",
            "simulation result: I'm so happy to be SHOOTING this\n",
            "reward: -217.8579864501953\n",
            "back\n",
            "parent: I'm so happy;   -399.10720011393227\n",
            "parent: I'm so happy to;   -230.1177659034729\n",
            "parent: I'm so happy to be;   -96.8660535812378\n",
            "parent: I'm so happy to be SH;   -131.71642303466797\n",
            "parent: I'm so happy to be SHO;   -217.8579864501953\n",
            "root: I'm so happy;   -399.10720011393227\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be typewritten.\n",
            "leaf node w/ WM:I'm so happy to be typewritten vol\n",
            "leaf node w/ WM:I'm so happy to be typewrittenBIT\n",
            "leaf node w/ WM:I'm so happy to be typewritteninsk\n",
            "leaf node w/ WM:I'm so happy to be typewritten uranium\n",
            "leaf node w/ WM:I'm so happy to be typewritten rampage\n",
            "simulation result: I'm so happy to be typewritten by the people\n",
            "reward: -465.8504333496094\n",
            "back\n",
            "parent: I'm so happy;   -401.2602076376638\n",
            "parent: I'm so happy to;   -243.98439340030447\n",
            "parent: I'm so happy to be;   -137.86431799994574\n",
            "parent: I'm so happy to be typew;   -266.69516372680664\n",
            "parent: I'm so happy to be typewritten;   -465.8504333496094\n",
            "root: I'm so happy;   -401.2602076376638\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to**********.\n",
            "leaf node w/ WM:I'm so happy to**********rio\n",
            "leaf node w/ WM:I'm so happy to********** scenario\n",
            "leaf node w/ WM:I'm so happy to********** Katie\n",
            "leaf node w/ WM:I'm so happy to**********see\n",
            "leaf node w/ WM:I'm so happy to********** abort\n",
            "simulation result: I'm so happy to**********! I'll\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -1199.17578125\n",
            "back\n",
            "parent: I'm so happy;   -426.1950693130493\n",
            "parent: I'm so happy to;   -297.05058161417645\n",
            "parent: I'm so happy to********;   -689.0046844482422\n",
            "parent: I'm so happy to**********;   -1199.17578125\n",
            "root: I'm so happy;   -426.1950693130493\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to embryo a baby\n",
            "leaf node w/ WM:I'm so happy to embryo agregation\n",
            "leaf node w/ WM:I'm so happy to embryo a 140\n",
            "leaf node w/ WM:I'm so happy to embryo a physical\n",
            "leaf node w/ WM:I'm so happy to embryo a intuition\n",
            "leaf node w/ WM:I'm so happy to embryo a Eval\n",
            "simulation result: I'm so happy to embryo a new family and\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -1647.4246826171875\n",
            "back\n",
            "parent: I'm so happy;   -463.2020272919626\n",
            "parent: I'm so happy to;   -368.1229027195981\n",
            "parent: I'm so happy to embryo;   -946.2312850952148\n",
            "parent: I'm so happy to embryo a;   -1647.4246826171875\n",
            "root: I'm so happy;   -463.2020272919626\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet Barbie.\n",
            "leaf node w/ WM:I'm so happy meet Barbie Mü\n",
            "leaf node w/ WM:I'm so happy meet Barbie Persona\n",
            "leaf node w/ WM:I'm so happy meet Barbie exiting\n",
            "leaf node w/ WM:I'm so happy meet Barbie Men\n",
            "leaf node w/ WM:I'm so happy meet Barbie played\n",
            "simulation result: I'm so happy meet Barbie and see you\n",
            "reward: -291.9096984863281\n",
            "back\n",
            "parent: I'm so happy;   -458.16401762120864\n",
            "parent: I'm so happy meet;   -327.5127857753209\n",
            "parent: I'm so happy meet Barbie;   -291.9096984863281\n",
            "root: I'm so happy;   -458.16401762120864\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy meet Orson Scott\n",
            "leaf node w/ WM:I'm so happy meet Orson covert\n",
            "leaf node w/ WM:I'm so happy meet Orson Interstellar\n",
            "leaf node w/ WM:I'm so happy meet Orson releg\n",
            "leaf node w/ WM:I'm so happy meet Orson Un\n",
            "leaf node w/ WM:I'm so happy meet Orson Original\n",
            "simulation result: I'm so happy meet Orson Welles and\n",
            "reward: -1032.6461181640625\n",
            "back\n",
            "parent: I'm so happy;   -474.57779192243305\n",
            "parent: I'm so happy meet;   -415.6544523239136\n",
            "parent: I'm so happy meet Or;   -622.1985092163086\n",
            "parent: I'm so happy meet Orson;   -1032.6461181640625\n",
            "root: I'm so happy;   -474.57779192243305\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to beamer. I\n",
            "leaf node w/ WM:I'm so happy to beamer. hamstring\n",
            "leaf node w/ WM:I'm so happy to beamer. 373\n",
            "leaf node w/ WM:I'm so happy to beamer. Employees\n",
            "leaf node w/ WM:I'm so happy to beamer. homelessness\n",
            "leaf node w/ WM:I'm so happy to beamer.puters\n",
            "simulation result: I'm so happy to beamer. I think it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -370.3338317871094\n",
            "back\n",
            "parent: I'm so happy;   -471.68212636311847\n",
            "parent: I'm so happy to;   -368.2334491729736\n",
            "parent: I'm so happy to be;   -161.11126937866212\n",
            "parent: I'm so happy to beamer;   -220.7223014831543\n",
            "parent: I'm so happy to beamer.;   -370.3338317871094\n",
            "root: I'm so happy;   -471.68212636311847\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be teenage again.\n",
            "leaf node w/ WM:I'm so happy to be teenage again Left\n",
            "leaf node w/ WM:I'm so happy to be teenage againSpread\n",
            "leaf node w/ WM:I'm so happy to be teenage againowler\n",
            "leaf node w/ WM:I'm so happy to be teenage againevaluate\n",
            "leaf node w/ WM:I'm so happy to be teenage againumen\n",
            "simulation result: I'm so happy to be teenage again! I'll\n",
            "reward: -400.00299072265625\n",
            "back\n",
            "parent: I'm so happy;   -469.7448524268898\n",
            "parent: I'm so happy to;   -369.7462844848633\n",
            "parent: I'm so happy to be;   -182.8286985917525\n",
            "parent: I'm so happy to be teenage;   -235.81707382202148\n",
            "parent: I'm so happy to be teenage again;   -400.00299072265625\n",
            "root: I'm so happy;   -469.7448524268898\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here. I\n",
            "leaf node w/ WM:I'm so happy to be here. Cheng\n",
            "leaf node w/ WM:I'm so happy to be here. heterosexual\n",
            "leaf node w/ WM:I'm so happy to be here. Legions\n",
            "leaf node w/ WM:I'm so happy to be here.nance\n",
            "leaf node w/ WM:I'm so happy to be here. automate\n",
            "simulation result: I'm so happy to be here. I think it\n",
            "reward: -82.58114624023438\n",
            "back\n",
            "parent: I'm so happy;   -459.5563338430304\n",
            "parent: I'm so happy to;   -356.693323655562\n",
            "parent: I'm so happy to be;   -174.47473589579263\n",
            "parent: I'm so happy to be here;   -82.3461799621582\n",
            "parent: I'm so happy to be here.;   -82.58114624023438\n",
            "root: I'm so happy;   -459.5563338430304\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be hereVisit the\n",
            "leaf node w/ WM:I'm so happy to be hereVisit Buc\n",
            "leaf node w/ WM:I'm so happy to be hereVisit Brav\n",
            "leaf node w/ WM:I'm so happy to be hereVisit Franchise\n",
            "leaf node w/ WM:I'm so happy to be hereVisit grappling\n",
            "leaf node w/ WM:I'm so happy to be hereVisit monsters\n",
            "simulation result: I'm so happy to be hereVisit the website of\n",
            "reward: -64.78301239013672\n",
            "back\n",
            "parent: I'm so happy;   -449.4339409852639\n",
            "parent: I'm so happy to;   -344.0015709918478\n",
            "parent: I'm so happy to be;   -166.0369110107422\n",
            "parent: I'm so happy to be here;   -76.49179077148438\n",
            "parent: I'm so happy to be hereVisit;   -64.78301239013672\n",
            "root: I'm so happy;   -449.4339409852639\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here deteriorating.\n",
            "leaf node w/ WM:I'm so happy to be here deteriorating Somali\n",
            "leaf node w/ WM:I'm so happy to be here deterioratingrers\n",
            "leaf node w/ WM:I'm so happy to be here deteriorating>\"\n",
            "leaf node w/ WM:I'm so happy to be here deterioratingung\n",
            "leaf node w/ WM:I'm so happy to be here deteriorating cited\n",
            "simulation result: I'm so happy to be here deteriorating in the middle\n",
            "reward: -70.69205474853516\n",
            "back\n",
            "parent: I'm so happy;   -439.9653938293457\n",
            "parent: I'm so happy to;   -332.6136744817098\n",
            "parent: I'm so happy to be;   -159.22656413487024\n",
            "parent: I'm so happy to be here;   -75.04185676574707\n",
            "parent: I'm so happy to be here deteriorating;   -70.69205474853516\n",
            "root: I'm so happy;   -439.9653938293457\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here Garmin.\n",
            "leaf node w/ WM:I'm so happy to be here Garminstitial\n",
            "leaf node w/ WM:I'm so happy to be here Garminwithout\n",
            "leaf node w/ WM:I'm so happy to be here Garmin RIP\n",
            "leaf node w/ WM:I'm so happy to be here Garmin determination\n",
            "leaf node w/ WM:I'm so happy to be here Garmin Kelly\n",
            "simulation result: I'm so happy to be here Garmin is helping me\n",
            "reward: -62.93354797363281\n",
            "back\n",
            "parent: I'm so happy;   -430.76949514993805\n",
            "parent: I'm so happy to;   -321.8264694213867\n",
            "parent: I'm so happy to be;   -152.8070297241211\n",
            "parent: I'm so happy to be here;   -72.62019500732421\n",
            "parent: I'm so happy to be here Garmin;   -62.93354797363281\n",
            "root: I'm so happy;   -430.76949514993805\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here Preservation is\n",
            "leaf node w/ WM:I'm so happy to be here Preservation 358\n",
            "leaf node w/ WM:I'm so happy to be here Preservation023\n",
            "leaf node w/ WM:I'm so happy to be here Preservationmanent\n",
            "leaf node w/ WM:I'm so happy to be here Preservationcourse\n",
            "leaf node w/ WM:I'm so happy to be here Preservationeties\n",
            "simulation result: I'm so happy to be here Preservation is going strong\n",
            "reward: -91.3983383178711\n",
            "back\n",
            "parent: I'm so happy;   -422.68922951107936\n",
            "parent: I'm so happy to;   -312.96384899432843\n",
            "parent: I'm so happy to be;   -148.96898651123047\n",
            "parent: I'm so happy to be here;   -75.74988555908203\n",
            "parent: I'm so happy to be here Preservation;   -91.3983383178711\n",
            "root: I'm so happy;   -422.68922951107936\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be here kidnapping you\n",
            "leaf node w/ WM:I'm so happy to be here kidnapping Eh\n",
            "leaf node w/ WM:I'm so happy to be here kidnapping patron\n",
            "leaf node w/ WM:I'm so happy to be here kidnapping Recovery\n",
            "leaf node w/ WM:I'm so happy to be here kidnappingzzy\n",
            "leaf node w/ WM:I'm so happy to be here kidnappingBrad\n",
            "simulation result: I'm so happy to be here kidnapping and taking away\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reward: -70.86495971679688\n",
            "back\n",
            "parent: I'm so happy;   -414.5072697484216\n",
            "parent: I'm so happy to;   -303.99722346553096\n",
            "parent: I'm so happy to be;   -144.37463199391084\n",
            "parent: I'm so happy to be here;   -75.05203901018415\n",
            "parent: I'm so happy to be here kidnapping;   -70.86495971679688\n",
            "root: I'm so happy;   -414.5072697484216\n",
            "simulation result: I'm so happy to be here Garmin.\n",
            "Posted by\n",
            "reward: -640.8636474609375\n",
            "back\n",
            "parent: I'm so happy;   -419.6517328782515\n",
            "parent: I'm so happy to;   -316.0281671796526\n",
            "parent: I'm so happy to be;   -171.95735507541232\n",
            "parent: I'm so happy to be here;   -145.77849006652832\n",
            "parent: I'm so happy to be here Garmin;   -351.89859771728516\n",
            "parent: I'm so happy to be here Garmin.;   -640.8636474609375\n",
            "root: I'm so happy;   -419.6517328782515\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to bepled to the\n",
            "leaf node w/ WM:I'm so happy to bepled toutations\n",
            "leaf node w/ WM:I'm so happy to bepled to replies\n",
            "leaf node w/ WM:I'm so happy to bepled to Forms\n",
            "leaf node w/ WM:I'm so happy to bepled to HERO\n",
            "leaf node w/ WM:I'm so happy to bepled to furnish\n",
            "simulation result: I'm so happy to bepled to the Church of\n",
            "reward: -651.2435913085938\n",
            "back\n",
            "parent: I'm so happy;   -424.798218621148\n",
            "parent: I'm so happy to;   -327.58731973582303\n",
            "parent: I'm so happy to be;   -197.1829464561061\n",
            "parent: I'm so happy to bepled;   -395.19378662109375\n",
            "parent: I'm so happy to bepled to;   -651.2435913085938\n",
            "root: I'm so happy;   -424.798218621148\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be SH explorer.\n",
            "leaf node w/ WM:I'm so happy to be SH explorerivating\n",
            "leaf node w/ WM:I'm so happy to be SH explorer nun\n",
            "leaf node w/ WM:I'm so happy to be SH explorerMpServer\n",
            "leaf node w/ WM:I'm so happy to be SH explorer subtly\n",
            "leaf node w/ WM:I'm so happy to be SH explorer air\n",
            "simulation result: I'm so happy to be SH explorer. I was\n",
            "reward: -218.46519470214844\n",
            "back\n",
            "parent: I'm so happy;   -420.31271810116976\n",
            "parent: I'm so happy to;   -323.9499155680339\n",
            "parent: I'm so happy to be;   -198.2470588684082\n",
            "parent: I'm so happy to be SH;   -160.63268025716147\n",
            "parent: I'm so happy to be SH explorer;   -218.46519470214844\n",
            "root: I'm so happy;   -420.31271810116976\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to be SH Died in\n",
            "leaf node w/ WM:I'm so happy to be SH Diedorgetown\n",
            "leaf node w/ WM:I'm so happy to be SH Died223\n",
            "leaf node w/ WM:I'm so happy to be SH Died fossil\n",
            "leaf node w/ WM:I'm so happy to be SH Died Og\n",
            "leaf node w/ WM:I'm so happy to be SH Died Design\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "simulation result: I'm so happy to be SH Died in the womb\n",
            "reward: -278.39971923828125\n",
            "back\n",
            "parent: I'm so happy;   -417.2932925934487\n",
            "parent: I'm so happy to;   -322.48055439610636\n",
            "parent: I'm so happy to be;   -202.06385221935454\n",
            "parent: I'm so happy to be SH;   -190.0744400024414\n",
            "parent: I'm so happy to be SH Died;   -278.39971923828125\n",
            "root: I'm so happy;   -417.2932925934487\n",
            "simulation result: I'm so happy to be hereVisit the website of a\n",
            "reward: -531.3255615234375\n",
            "back\n",
            "parent: I'm so happy;   -419.6689648628235\n",
            "parent: I'm so happy to;   -329.00696086883545\n",
            "parent: I'm so happy to be;   -217.0302935513583\n",
            "parent: I'm so happy to be here;   -188.61705356174045\n",
            "parent: I'm so happy to be hereVisit;   -298.0542869567871\n",
            "parent: I'm so happy to be hereVisit the;   -531.3255615234375\n",
            "root: I'm so happy;   -419.6689648628235\n",
            "expansion:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaf node w/o WM:I'm so happy to Actually pandering\n",
            "leaf node w/ WM:I'm so happy to Actually pand booze\n",
            "leaf node w/ WM:I'm so happy to Actually pand Whale\n",
            "leaf node w/ WM:I'm so happy to Actually pand laughed\n",
            "leaf node w/ WM:I'm so happy to Actually pand Orders\n",
            "leaf node w/ WM:I'm so happy to Actually pand knight\n",
            "simulation result: I'm so happy to Actually pandering and not\n",
            "reward: -322.27783203125\n",
            "back\n",
            "parent: I'm so happy;   -417.68139072340364\n",
            "parent: I'm so happy to;   -328.8030478737571\n",
            "parent: I'm so happy to Actually;   -311.20716094970703\n",
            "parent: I'm so happy to Actually pand;   -322.27783203125\n",
            "baseline response: I'm so happy to be here. I've been working\n",
            "Best response with watermark: I'm so happy to be here deteriorating.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cacZvcznuL_s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}